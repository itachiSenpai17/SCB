{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class PolicyCNN(object):\n",
    "    '''\n",
    "    Using this class we will build policy for the cnn network.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, ohlc_feature_num, ticker_num, num_trading_periods, sess, optimizer, trading_cost, cash_bias_init, interest_rate, \n",
    "        equiweight_vector, adjusted_rewards_alpha, kernel_size, num_filter_layer_1, num_filter_layer_2):\n",
    "\n",
    "        # parameters\n",
    "        self.ohlc_feature_num = ohlc_feature_num\n",
    "        self.ticker_num = ticker_num\n",
    "        self. num_trading_periods =  num_trading_periods\n",
    "        self.trading_cost = trading_cost\n",
    "        self.cash_bias_init = cash_bias_init\n",
    "        self.interest_rate = interest_rate\n",
    "        self.equiweight_vector = equiweight_vector\n",
    "        self.adjusted_rewards_alpha = adjusted_rewards_alpha \n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_filter_layer_1 = num_filter_layer_1\n",
    "        self.num_filter_layer_2 = num_filter_layer_2\n",
    "        self.optimizer = optimizer\n",
    "        self.sess = sess\n",
    "\n",
    "        self.X_t = tf.placeholder(tf.float32, [None, self.ohlc_feature_num, self.ticker_num, self.num_trading_periods])\n",
    "        self.weights_previous_t = tf.placeholder(tf.float32, [None, self.ticker_num + 1])\n",
    "        self.pf_previous_t = tf.placeholder(tf.float32, [None, 1])\n",
    "        self.daily_returns_t = tf.placeholder(tf.float32, [None, self.ticker_num]) \n",
    "        cash_bias = tf.get_variable('cash_bias', shape=[1, 1, 1, 1], initializer = tf.constant_initializer(self.cash_bias_init))\n",
    "        shape_X_t = tf.shape(self.X_t)[0]\n",
    "        self.cash_bias = tf.tile(cash_bias, tf.stack([shape_X_t, 1, 1, 1]))\n",
    "\n",
    "        def convolution_layers(X_t, num_filter_layer_1, kernel_size, num_filter_layer_2, num_trading_periods):\n",
    "            with tf.variable_scope(\"Convolution1\"):\n",
    "                convolution1 = tf.layers.conv2d(\n",
    "                inputs = tf.transpose(X_t, perm=[0, 3, 2, 1]),\n",
    "                activation = tf.nn.tanh,\n",
    "                filters = num_filter_layer_1,\n",
    "                strides = (1, 1),\n",
    "                kernel_size = kernel_size,\n",
    "                padding = 'same')\n",
    "\n",
    "\n",
    "            with tf.variable_scope(\"Convolution2\"):\n",
    "                convolution2 = tf.layers.conv2d(\n",
    "                inputs = convolution1,\n",
    "                activation = tf.nn.tanh,\n",
    "                filters = num_filter_layer_2,\n",
    "                strides = (num_trading_periods, 1),\n",
    "                kernel_size = (1, num_trading_periods),\n",
    "                padding = 'same')\n",
    "\n",
    "            with tf.variable_scope(\"Convolution3\"):\n",
    "                self.convolution3 = tf.layers.conv2d(\n",
    "                    inputs = convolution2,\n",
    "                    activation = tf.nn.relu,\n",
    "                    filters = 1,\n",
    "                    strides = (num_filter_layer_2 + 1, 1),\n",
    "                    kernel_size = (1, 1),\n",
    "                    padding = 'same')\n",
    "\n",
    "            return self.convolution3\n",
    "\n",
    "\n",
    "        def policy_output(convolution, cash_bias):\n",
    "            with tf.variable_scope(\"Policy-Output\"):\n",
    "                tensor_squeeze = tf.squeeze(tf.concat([cash_bias, convolution], axis=2), [1, 3])\n",
    "                self.action = tf.nn.softmax(tensor_squeeze)\n",
    "            return self.action\n",
    "\n",
    "\n",
    "        def reward(shape_X_t, action_chosen, interest_rate, weights_previous_t, pf_previous_t, daily_returns_t, trading_cost):\n",
    "            #Calculating reward for current Portfolio\n",
    "            with tf.variable_scope(\"Reward\"):\n",
    "                cash_return = tf.tile(tf.constant(1 + interest_rate, shape=[1, 1]), tf.stack([shape_X_t, 1]))\n",
    "                y_t = tf.concat([cash_return, daily_returns_t], axis=1)\n",
    "                pf_vector_t = action_chosen * pf_previous_t\n",
    "                pf_vector_previous = weights_previous_t * pf_previous_t\n",
    "\n",
    "                total_trading_cost = trading_cost * tf.norm(pf_vector_t - pf_vector_previous, ord=1, axis=1) * tf.constant(1.0, shape=[1])\n",
    "                total_trading_cost = tf.expand_dims(total_trading_cost, 1)\n",
    "\n",
    "                zero_vector = tf.tile(tf.constant(np.array([0.0] * ticker_num).reshape(1, ticker_num), shape=[1, ticker_num], dtype=tf.float32), tf.stack([shape_X_t, 1]))\n",
    "                cost_vector = tf.concat([total_trading_cost, zero_vector], axis=1)\n",
    "\n",
    "                pf_vector_second_t = pf_vector_t - cost_vector\n",
    "                final_pf_vector_t = tf.multiply(pf_vector_second_t, y_t)\n",
    "                portfolio_value = tf.norm(final_pf_vector_t, ord=1)\n",
    "                self.instantaneous_reward = (portfolio_value - pf_previous_t) / pf_previous_t\n",
    "                \n",
    "            #Calculating Reward for Equiweight portfolio\n",
    "            with tf.variable_scope(\"Reward-Equiweighted\"):\n",
    "                cash_return = tf.tile(tf.constant(1 + interest_rate, shape=[1, 1]), tf.stack([shape_X_t, 1]))\n",
    "                y_t = tf.concat([cash_return, daily_returns_t], axis=1)\n",
    "  \n",
    "                pf_vector_eq = self.equiweight_vector * pf_previous_t\n",
    "        \n",
    "                portfolio_value_eq = tf.norm(tf.multiply(pf_vector_eq, y_t), ord=1)\n",
    "                self.instantaneous_reward_eq = (portfolio_value_eq - pf_previous_t) / pf_previous_t\n",
    "\n",
    "            #Calculating Adjusted Rewards\n",
    "            with tf.variable_scope(\"Reward-adjusted\"):\n",
    "                self.adjusted_reward = self.instantaneous_reward - self.instantaneous_reward_eq - self.adjusted_rewards_alpha * tf.reduce_max(action_chosen)\n",
    "                \n",
    "            return self.adjusted_reward\n",
    "\n",
    "\n",
    "        self.convolution = convolution_layers(self.X_t, self.num_filter_layer_1, self.kernel_size, self.num_filter_layer_2, self.num_trading_periods) \n",
    "        self.action_chosen = policy_output(self.convolution, self.cash_bias)\n",
    "        self.adjusted_reward = reward(shape_X_t, self.action_chosen, self.interest_rate, self.weights_previous_t, self.pf_previous_t, self.daily_returns_t, self.trading_cost)\n",
    "        self.train_op = optimizer.minimize(-self.adjusted_reward)\n",
    "\n",
    "    def compute_weights(self, X_t_, weights_previous_t_):\n",
    "        return self.sess.run(tf.squeeze(self.action_chosen), feed_dict={self.X_t: X_t_, self.weights_previous_t: weights_previous_t_})\n",
    "\n",
    "    def train_cnn(self, X_t_, weights_previous_t_, pf_previous_t_, daily_returns_t_):\n",
    "        \"\"\"\n",
    "        training the neural network\n",
    "        \"\"\"\n",
    "        self.sess.run(self.train_op, feed_dict={self.X_t: X_t_,\n",
    "                                                self.weights_previous_t: weights_previous_t_,\n",
    "                                                self.pf_previous_t: pf_previous_t_,\n",
    "                                                self.daily_returns_t: daily_returns_t_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
