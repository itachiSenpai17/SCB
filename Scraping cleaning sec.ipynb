{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import requests\n",
    "import bs4 as bs\n",
    "from lxml import html\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "tickers = []\n",
    "for row in table.findAll('tr')[1:]:\n",
    "    ticker = row.findAll('td')[1].text\n",
    "    tickers.append(ticker)\n",
    "tickers = tickers[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.04it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 1 elements, new values have 2 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ee1f95993375>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mciks_tick_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_cik\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mciks_tick_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mciks_tick_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'ticker'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cik'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mciks_tick_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'AllSecTickers.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   5190\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5191\u001b[0m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5192\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5193\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5194\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[1;34m(self, axis, labels)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m    181\u001b[0m             raise ValueError(\n\u001b[0;32m    182\u001b[0m                 \u001b[1;34m\"Length mismatch: Expected axis has {old} elements, new \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m                 \u001b[1;34m\"values have {new} elements\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mold_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m             )\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length mismatch: Expected axis has 1 elements, new values have 2 elements"
     ]
    }
   ],
   "source": [
    "url = 'http://www.sec.gov/cgi-bin/browse-edgar?CIK={}&Find=Search&owner=exclude&action=getcompany'\n",
    "cik_re = re.compile(r'.*CIK=(\\d{10}).*')\n",
    "all_cik = {}\n",
    "for ticker in tqdm(tickers):\n",
    "    res = cik_re.findall(requests.get(url.format(ticker)).text)\n",
    "    if len(res):\n",
    "        all_cik[str(ticker).lower()] = str(res[0])\n",
    "        \n",
    "ciks_tick_df = pd.DataFrame.from_dict(data=all_cik, orient='index')\n",
    "ciks_tick_df.reset_index(inplace=True)\n",
    "ciks_tick_df.columns = ['ticker', 'cik']\n",
    "\n",
    "ciks_tick_df.to_csv('AllSecTickers.csv', sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveLogFile(log_file_name, text):\n",
    "    \n",
    "    with open(log_file_name, \"a\") as log_file:\n",
    "        log_file.write(text)\n",
    "\n",
    "    return\n",
    "#closing log file\n",
    "def CloseLogFile(log_file_name):\n",
    "    \n",
    "    with open(log_file_name, 'a') as log_file:\n",
    "        log_file.close()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped10k_path = \"C:/Users/Tanay/SCB Intern Project/Portfolio Construction/Data/scraped_10k\"\n",
    "scraped10q_path = \"C:/Users/Tanay/SCB Intern Project/Portfolio Construction/Data/scraped_10q\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract10K(browse_url_base, filing_url_base, doc_url_base, cik, log_file_name):\n",
    "    \n",
    "    requestcount=0\n",
    "    if not os.path.exists(cik):\n",
    "        os.makedirs(cik)\n",
    "    \n",
    "    os.chdir(cik)\n",
    "    print('Scraping CIK ', cik)\n",
    "    \n",
    "    SaveLogFile(log_file_name, 'Scraping CIK: '+cik)\n",
    "            \n",
    "    res = requests.get(browse_url_base % cik)\n",
    "    requestcount+=1\n",
    "    if(requestcount==10):\n",
    "        time.sleep(1)\n",
    "        requestcount=0\n",
    "\n",
    "    if res.status_code != 200:\n",
    "        text = \"\\nFailed at step 1.\\n Failed to hit browse base URL for CIK with error \" + str(res.status_code) + \\\n",
    "               \"\\nFailed URL: \" + (browse_url_base % cik) + '\\n'\n",
    "        \n",
    "        SaveLogFile(log_file_name, text)\n",
    "            \n",
    "        return\n",
    "\n",
    "    SaveLogFile(log_file_name,\"\\nSuccessfully reached browse base UrL\\n\")\n",
    "        \n",
    "    #HTML parsing using BeautifulSoup\n",
    "    soup = bs.BeautifulSoup(res.text, \"lxml\")\n",
    "\n",
    " \n",
    "    html_tables = soup.find_all('table')\n",
    "    \n",
    "    if len(html_tables)<3:\n",
    "        os.chdir('..')\n",
    "        return\n",
    "    \n",
    "\n",
    "    sec_filing_tbl = pd.read_html(str(html_tables[2]), header=0)[0]\n",
    "    sec_filing_tbl['Filings'] = [str(x) for x in sec_filing_tbl['Filings']]\n",
    "\n",
    "    sec_filing_tbl = sec_filing_tbl[(sec_filing_tbl['Filings'] == '10-K') | (sec_filing_tbl['Filings'] == '10-K405')]\n",
    "    sec_filing_tbl = sec_filing_tbl[(sec_filing_tbl['Filing Date'] >= '2008-01-01')]\n",
    "\n",
    "    if len(sec_filing_tbl)==0:\n",
    "        os.chdir('..')\n",
    "        return\n",
    "    \n",
    "    sec_filing_tbl['Acc_No'] = [x.replace('\\xa0',' ')\n",
    "                               .split('Acc-no: ')[1]\n",
    "                               .split(' ')[0] for x in sec_filing_tbl['Description']]\n",
    "\n",
    "   \n",
    "    SaveLogFile(log_file_name, \"Getting all documents for this CIK\\n\")\n",
    "    \n",
    "    total_files_to_scrape = len(sec_filing_tbl)\n",
    "    files_scraped = 0\n",
    "    \n",
    "    for index, row in sec_filing_tbl.iterrows():\n",
    "        \n",
    "       \n",
    "        acc_no = str(row['Acc_No'])\n",
    "        \n",
    "        #check if file is already scraped \n",
    "        date = str(row['Filing Date'])\n",
    "        if (os.path.exists(cik + '_' + date + '.html')) or os.path.exists(cik + '_' + date + '.txt'):\n",
    "            SaveLogFile(log_file_name, \"The file for date: \" + date + \" exists, acc no.: \" + acc_no + \"\\n\")\n",
    "            files_scraped+=1\n",
    "            continue\n",
    "            \n",
    "   \n",
    "        docs_page = requests.get(filing_url_base % (cik, acc_no))\n",
    "        requestcount+=1\n",
    "        if(requestcount==10):\n",
    "            time.sleep(1)\n",
    "            requestcount=0\n",
    "        \n",
    "        \n",
    "        #In case of request failure,log it and jump to the next filing \n",
    "        if docs_page.status_code != 200:\n",
    "            text = \"Request failed with error code \" + str(docs_page.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (filing_url_base % (cik, acc_no)) + '\\n'\n",
    "            SaveLogFile(log_file_name, text)\n",
    "            continue\n",
    "            \n",
    "        SaveLogFile(log_file_name, \"Got acc no. \" + acc_no + \"\\n\")\n",
    "       \n",
    "        docs_page_soup = bs.BeautifulSoup(docs_page.text, 'lxml')\n",
    "        docs_html_tables = docs_page_soup.find_all('table')\n",
    "        if len(docs_html_tables)==0:\n",
    "            continue\n",
    "        docs_table = pd.read_html(str(docs_html_tables[0]), header=0)[0]\n",
    "        docs_table['Type'] = [str(x) for x in docs_table['Type']]\n",
    "        \n",
    " \n",
    "        docs_table = docs_table[(docs_table['Type'] == '10-K') | (docs_table['Type'] == '10-K405')]\n",
    "        \n",
    "        if len(docs_table)==0:\n",
    "            continue\n",
    "        elif len(docs_table)>0:\n",
    "            docs_table = docs_table.iloc[0]\n",
    "        \n",
    "        docname = docs_table['Document']\n",
    "        \n",
    "         # check for nan values, if present then log the failure\n",
    "        if str(docname) == 'nan':\n",
    "            text = 'File with CIK: %s and Acc_No: %s is unavailable' % (cik, acc_no) + '\\n'\n",
    "            SaveLogFile(log_file_name, text)\n",
    "            continue       \n",
    "        \n",
    "        file = requests.get(doc_url_base % (cik, acc_no.replace('-', ''), docname.replace(' iXBRL','')))\n",
    "        requestcount+=1\n",
    "        if(requestcount==10):\n",
    "            time.sleep(1)\n",
    "            requestcount=0\n",
    "            \n",
    "       \n",
    "        if file.status_code != 200:\n",
    "            os.chdir('..')\n",
    "            text = \"Request failed with error code \" + str(file.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (doc_url_base % (cik, acc_no.replace('-', ''), docname)) + '\\n'\n",
    "            SaveLogFile(log_file_name, text)\n",
    "            os.chdir(cik)\n",
    "            continue\n",
    "        \n",
    "        SaveLogFile(log_file_name, \"Got 10K for acc no. \" + acc_no + \"\\n\")\n",
    "        files_scraped+=1\n",
    "        \n",
    "        #save HTML or Text file with .txt extension\n",
    "        if '.txt' in docname:\n",
    "            filename = cik + '_' + date + '.txt'\n",
    "            html_file = open(filename, 'a')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "        else:\n",
    "            filename = cik + '_' + date + '.html'\n",
    "            html_file = open(filename, 'a')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "    \n",
    "    if(total_files_to_scrape!=files_scraped):\n",
    "        print(\"Some files failed to scrape\\n\")\n",
    "        text=\"Some files failed to scrape\\n\" + str(total_files_to_scrape) + \"!=\"+ str(files_scraped) + \"\\n\"\n",
    "        SaveLogFile(log_file_name, text)\n",
    "        \n",
    "    SaveLogFile(log_file_name, \"saving log files=================\\n\")\n",
    "    CloseLogFile(log_file_name)\n",
    "    os.chdir('..')\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract10Q(browse_url_base, filing_url_base, doc_url_base, cik, log_file_name):\n",
    "    \n",
    "    requestcount=0\n",
    "\n",
    "    if not os.path.exists(cik):\n",
    "        os.makedirs(cik)\n",
    "    \n",
    "    os.chdir(cik)\n",
    "    print('Scraping CIK ', cik)\n",
    "    \n",
    "    SaveLogFile(log_file_name, 'Scraping CIK: '+cik)\n",
    "            \n",
    "    res = requests.get(browse_url_base % cik)\n",
    "    requestcount+=1\n",
    "    if(requestcount==10):\n",
    "        time.sleep(1)\n",
    "        requestcount=0\n",
    "    \n",
    "     #In case of request failure,log it and jump to the next filing \n",
    "    if res.status_code != 200:\n",
    "        text = \"\\nFailed at step 1.\\n Failed to hit browse base URL for CIK with error \" + str(res.status_code) + \\\n",
    "               \"\\nFailed URL: \" + (browse_url_base % cik) + '\\n'\n",
    "        \n",
    "        SaveLogFile(log_file_name, text)\n",
    "            \n",
    "        return\n",
    "\n",
    "   \n",
    "    SaveLogFile(log_file_name,\"\\nSuccessfully reached browse base UrL\\n\")\n",
    "        \n",
    "    soup = bs.BeautifulSoup(res.text, \"lxml\")\n",
    "\n",
    "    # fetching all the html tables\n",
    "    html_tables = soup.find_all('table')\n",
    "    \n",
    "    \n",
    "    if len(html_tables)<3:\n",
    "        os.chdir('..')\n",
    "        return\n",
    "    \n",
    "    # Parsing the html filing table\n",
    "    sec_filing_tbl = pd.read_html(str(html_tables[2]), header=0)[0]\n",
    "    sec_filing_tbl['Filings'] = [str(x) for x in sec_filing_tbl['Filings']]\n",
    "\n",
    "\n",
    "    sec_filing_tbl = sec_filing_tbl[sec_filing_tbl['Filings'] == '10-Q']\n",
    "    sec_filing_tbl = sec_filing_tbl[(sec_filing_tbl['Filing Date'] >= '2008-01-01')]\n",
    "    \n",
    "    if len(sec_filing_tbl)==0:\n",
    "        os.chdir('..')\n",
    "        return\n",
    "    \n",
    "  \n",
    "    sec_filing_tbl['Acc_No'] = [x.replace('\\xa0',' ')\n",
    "                               .split('Acc-no: ')[1]\n",
    "                               .split(' ')[0] for x in sec_filing_tbl['Description']]\n",
    "\n",
    "   \n",
    "    SaveLogFile(log_file_name, \"Getting all documents for this CIK\\n\")\n",
    "    \n",
    "    total_files_to_scrape = len(sec_filing_tbl)\n",
    "    files_scraped = 0\n",
    "    \n",
    "    for index, row in sec_filing_tbl.iterrows():\n",
    "        \n",
    "     \n",
    "        acc_no = str(row['Acc_No'])\n",
    "        \n",
    "        # check if file already exist\n",
    "        date = str(row['Filing Date'])\n",
    "        if (os.path.exists(cik + '_' + date + '.html')) or os.path.exists(cik + '_' + date + '.txt'):\n",
    "            SaveLogFile(log_file_name, \"The file for date: \" + date + \" exists, acc no.: \" + acc_no + \"\\n\")\n",
    "            files_scraped+=1\n",
    "            continue\n",
    "            \n",
    "      \n",
    "        docs_page = requests.get(filing_url_base % (cik, acc_no))\n",
    "        requestcount+=1\n",
    "        if(requestcount==10):\n",
    "            time.sleep(1)\n",
    "            requestcount=0\n",
    "        \n",
    "        #In case of request failure,log it and jump to the next filing \n",
    "        if docs_page.status_code != 200:\n",
    "            text = \"Request failed with error code \" + str(docs_page.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (filing_url_base % (cik, acc_no)) + '\\n'\n",
    "            SaveLogFile(log_file_name, text)\n",
    "            continue\n",
    "            \n",
    "        SaveLogFile(log_file_name, \"Got acc no. \" + acc_no + \"\\n\")\n",
    "\n",
    "        docs_page_soup = bs.BeautifulSoup(docs_page.text, 'lxml')\n",
    "        docs_html_tables = docs_page_soup.find_all('table')\n",
    "        \n",
    "        if len(docs_html_tables)==0:\n",
    "            continue\n",
    "            \n",
    "        docs_table = pd.read_html(str(docs_html_tables[0]), header=0)[0]\n",
    "        docs_table['Type'] = [str(x) for x in docs_table['Type']]\n",
    "        \n",
    "        docs_table = docs_table[docs_table['Type'] == '10-Q']\n",
    "        \n",
    "        if len(docs_table)==0:\n",
    "            continue\n",
    "            \n",
    "        elif len(docs_table)>0:\n",
    "            docs_table = docs_table.iloc[0]\n",
    "        \n",
    "        docname = docs_table['Document']\n",
    "    \n",
    "        # check for nan values, if present then log the failure\n",
    "        if str(docname) == 'nan':\n",
    "            text = 'File with CIK: %s and Acc_No: %s is unavailable' % (cik, acc_no) + '\\n'\n",
    "            SaveLogFile(log_file_name, text)\n",
    "            continue       \n",
    "        \n",
    "       \n",
    "        file = requests.get(doc_url_base % (cik, acc_no.replace('-', ''), docname.replace(' iXBRL','')))\n",
    "        requestcount+=1\n",
    "        if(requestcount==10):\n",
    "            time.sleep(1)\n",
    "            requestcount=0\n",
    "            \n",
    "        #In case of request failure,log it and exit\n",
    "        if file.status_code != 200:\n",
    "            os.chdir('..')\n",
    "            text = \"Request failed with error code \" + str(file.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (doc_url_base % (cik, acc_no.replace('-', ''), docname)) + '\\n'\n",
    "            SaveLogFile(log_file_name, text)\n",
    "            os.chdir(cik)\n",
    "            continue\n",
    "        \n",
    "      \n",
    "        SaveLogFile(log_file_name, \"Got 10Q for acc no. \" + acc_no + \"\\n\")\n",
    "        files_scraped+=1\n",
    "        \n",
    "         #save HTML or Text file with .txt extension\n",
    "        if '.txt' in docname:\n",
    "            filename = cik + '_' + date + '.txt'\n",
    "            html_file = open(filename, 'a')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "        else:\n",
    "            filename = cik + '_' + date + '.html'\n",
    "            html_file = open(filename, 'a')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "    \n",
    "    if(total_files_to_scrape!=files_scraped):\n",
    "        print(\"Some files failed to scrape\\n\")\n",
    "        text=\"Some files failed to scrape\\n\" + str(total_files_to_scrape) + \"!=\"+ str(files_scraped) + \"\\n\"\n",
    "        SaveLogFile(log_file_name, text)\n",
    "        \n",
    "    SaveLogFile(log_file_name, \"saving log files==========================\\n\")\n",
    "    CloseLogFile(log_file_name)\n",
    "    os.chdir('..')\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cik'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'cik'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-184463491f46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mlog_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'log.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mcik\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mciks_tick_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cik'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     Extract10K(browse_url_base=browse_url_10k, \n\u001b[0;32m     12\u001b[0m           \u001b[0mfiling_url_base\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfiling_url_10k\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2978\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2979\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2980\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2981\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2982\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2899\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'cik'"
     ]
    }
   ],
   "source": [
    "# Define parameters for Extract10K method\n",
    "browse_url_10k = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=%s&type=10-K'\n",
    "filing_url_10k = 'http://www.sec.gov/Archives/edgar/data/%s/%s-index.html'\n",
    "doc_url_10k = 'http://www.sec.gov/Archives/edgar/data/%s/%s/%s'\n",
    "\n",
    "# moving the directory, where scraped 10-K filings are\n",
    "os.chdir(scraped10k_path)\n",
    "log_file = 'log.txt'\n",
    "\n",
    "for cik in tqdm(ciks_tick_df['cik']):\n",
    "    Extract10K(browse_url_base=browse_url_10k, \n",
    "          filing_url_base=filing_url_10k, \n",
    "          doc_url_base=doc_url_10k, \n",
    "          cik=cik,\n",
    "          log_file_name=log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To scrape 10-Ks, call the Extract10Q method\n",
    "# Define parameters for Extract10Q method\n",
    "browse_url_10q = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=%s&type=10-Q&count=1000'\n",
    "filing_url_10q = 'http://www.sec.gov/Archives/edgar/data/%s/%s-index.html'\n",
    "doc_url_10q = 'http://www.sec.gov/Archives/edgar/data/%s/%s/%s'\n",
    "\n",
    "# Set correct directory (fill this out yourself!)\n",
    "os.chdir(scraped10q_path)\n",
    "log_file_name = 'log.txt'\n",
    "\n",
    "# moving the directory, where scraped 10-K filings are\n",
    "for cik in tqdm(ciks_tick_df['cik']):\n",
    "    Extract10Q(browse_url_base=browse_url_10q, \n",
    "          filing_url_base=filing_url_10q, \n",
    "          doc_url_base=doc_url_10q, \n",
    "          cik=cik,\n",
    "          log_file_name=log_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
